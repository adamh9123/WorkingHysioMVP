// OpenAI API integration for GPT-4o

import OpenAI from 'openai';
import type { AIResponse } from '@/lib/types';

// Initialize OpenAI client
let openaiClient: OpenAI | null = null;

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY environment variable is not set');
    }
    openaiClient = new OpenAI({ apiKey });
  }
  return openaiClient;
}

export interface OpenAICompletionOptions {
  model?: string; // Default to gpt-4o
  temperature?: number; // 0-2, controls randomness
  max_tokens?: number; // Maximum tokens in response
  top_p?: number; // 0-1, nucleus sampling
  frequency_penalty?: number; // -2 to 2, penalize frequent tokens
  presence_penalty?: number; // -2 to 2, penalize existing tokens
  stop?: string | string[]; // Stop sequences
  stream?: boolean; // Enable streaming
  user?: string; // Unique user identifier
}

export async function generateContentWithOpenAI(
  systemPrompt: string,
  userPrompt: string,
  options: OpenAICompletionOptions = {}
): Promise<AIResponse> {
  try {
    const {
      model = 'gpt-4o',
      temperature = 0.7,
      max_tokens = 2000,
      top_p = 1.0,
      frequency_penalty = 0,
      presence_penalty = 0,
      stop,
      user,
    } = options;

    // Get OpenAI client
    const client = getOpenAIClient();

    // Create completion
    const completion = await client.chat.completions.create({
      model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature,
      max_tokens,
      top_p,
      frequency_penalty,
      presence_penalty,
      stop,
      user,
    });

    const choice = completion.choices[0];
    
    if (!choice || !choice.message?.content) {
      throw new Error('No content generated by OpenAI');
    }

    return {
      success: true,
      content: choice.message.content.trim(),
      model: completion.model,
      usage: completion.usage ? {
        prompt_tokens: completion.usage.prompt_tokens,
        completion_tokens: completion.usage.completion_tokens,
        total_tokens: completion.usage.total_tokens,
      } : undefined,
    };

  } catch (error) {
    console.error('OpenAI completion error:', error);
    
    let errorMessage = 'Failed to generate content';
    
    if (error instanceof OpenAI.APIError) {
      errorMessage = `OpenAI API error: ${error.message}`;
      
      // Handle specific error types
      if (error.status === 429) {
        errorMessage = 'Rate limit exceeded. Please try again later.';
      } else if (error.status === 401) {
        errorMessage = 'Invalid OpenAI API key.';
      } else if (error.status === 402) {
        errorMessage = 'Insufficient OpenAI credits.';
      } else if (error.status === 503) {
        errorMessage = 'OpenAI service temporarily unavailable.';
      }
    } else if (error instanceof Error) {
      errorMessage = `OpenAI error: ${error.message}`;
    }

    return {
      success: false,
      error: errorMessage,
    };
  }
}

export interface OpenAIStreamOptions extends OpenAICompletionOptions {
  onChunk?: (chunk: string) => void;
  onComplete?: (fullContent: string) => void;
  onError?: (error: string) => void;
}

export async function generateContentStreamWithOpenAI(
  systemPrompt: string,
  userPrompt: string,
  options: OpenAIStreamOptions = {}
): Promise<AIResponse> {
  try {
    const {
      model = 'gpt-4o',
      temperature = 0.7,
      max_tokens = 2000,
      top_p = 1.0,
      frequency_penalty = 0,
      presence_penalty = 0,
      stop,
      user,
      onChunk,
      onComplete,
      onError,
    } = options;

    // Get OpenAI client
    const client = getOpenAIClient();

    // Create streaming completion
    const stream = await client.chat.completions.create({
      model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature,
      max_tokens,
      top_p,
      frequency_penalty,
      presence_penalty,
      stop,
      user,
      stream: true,
    });

    let fullContent = '';
    let usage: any = undefined;
    let modelUsed = model;

    try {
      for await (const chunk of stream) {
        const choice = chunk.choices[0];
        
        if (choice?.delta?.content) {
          const content = choice.delta.content;
          fullContent += content;
          onChunk?.(content);
        }
        
        // Update model and usage from chunk
        if (chunk.model) {
          modelUsed = chunk.model;
        }
        if (chunk.usage) {
          usage = chunk.usage;
        }
      }
      
      onComplete?.(fullContent);

      return {
        success: true,
        content: fullContent.trim(),
        model: modelUsed,
        usage: usage ? {
          prompt_tokens: usage.prompt_tokens,
          completion_tokens: usage.completion_tokens,
          total_tokens: usage.total_tokens,
        } : undefined,
      };

    } catch (streamError) {
      const errorMessage = streamError instanceof Error ? streamError.message : 'Stream processing error';
      onError?.(errorMessage);
      throw streamError;
    }

  } catch (error) {
    console.error('OpenAI streaming error:', error);
    
    let errorMessage = 'Failed to generate streaming content';
    
    if (error instanceof OpenAI.APIError) {
      errorMessage = `OpenAI streaming error: ${error.message}`;
    } else if (error instanceof Error) {
      errorMessage = `OpenAI streaming error: ${error.message}`;
    }

    options.onError?.(errorMessage);

    return {
      success: false,
      error: errorMessage,
    };
  }
}

// Function to estimate token count (rough approximation)
export function estimateTokenCount(text: string): number {
  // Rough approximation: 1 token â‰ˆ 4 characters for English text
  // This is not accurate but gives a ballpark estimate
  return Math.ceil(text.length / 4);
}

// Function to estimate cost (approximate)
export function estimateCompletionCost(
  promptTokens: number, 
  completionTokens: number, 
  model: string = 'gpt-4o'
): number {
  // GPT-4o pricing (as of 2024 - check OpenAI pricing for current rates)
  // Input: $0.0025 per 1K tokens
  // Output: $0.01 per 1K tokens
  const inputCost = (promptTokens / 1000) * 0.0025;
  const outputCost = (completionTokens / 1000) * 0.01;
  
  return inputCost + outputCost;
}

// Utility function to validate model availability
export function isValidOpenAIModel(model: string): boolean {
  const validModels = [
    'gpt-4o',
    'gpt-4o-mini',
    'gpt-4',
    'gpt-4-turbo',
    'gpt-3.5-turbo',
  ];
  
  return validModels.includes(model);
}